import logging
import os
import json
import re

from tqdm import tqdm
from sklearn.model_selection import train_test_split

from utils import string_process


logger = logging.getLogger(__name__)


# This is only for BERT model.
# Because this part does not limit the number of article
# , if BART model use the data generated by this part
# , the performance of model will decrease.
def get_common_data(parameters):
    logger.info(f'Start to get common data.')

    articles_list = []

    for article in parameters['articles_times_appeared_of_all_files']:
        if int(article[1]) >= parameters['article_lowerbound']:
            articles_list.append(article[0])

    data = []
    article_sources_list = []
    accusations_list = []

    for file_name in os.listdir(path=parameters['data_path']):
        if file_name == 'README.md':
            continue

        with open(
                file=os.path.join(parameters['data_path'], file_name)
                , mode='r'
                , encoding='UTF-8') as json_file:
            lines = json_file.readlines()

            for line in lines:
                data.append(line)

                item = json.loads(line)

                for relevant_article in item['meta']['relevant_articles']:
                    article = relevant_article[0] + relevant_article[1]
                    article_source = relevant_article[0]
                    accusation = item['meta']['accusation']

                    if article in articles_list:
                        if article_source not in article_sources_list \
                                and article_source != '':
                            article_sources_list.append(article_source)

                        if accusation not in accusations_list \
                                and accusation != '':
                            accusations_list.append(accusation)

            json_file.close()

    write_back_results(
        parameters=parameters
        , data=data
        , articles_list=articles_list
        , article_sources_list=article_sources_list
        , accusations_list=accusations_list
    )

    logger.info(f'Get common data successfully.')


def get_bart_summary(parameters):
    logger.info('Start to convert fact to summarization.')

    formatter = parameters['formatter']
    model = parameters['model']
    data = []

    exclusion_file_names = [
        'README.md'
        , 'articles.txt'
        , 'article_sources.txt'
        , 'accusations.txt'
    ]

    for file_name in os.listdir(path=parameters['data_path']):
        if file_name in exclusion_file_names:
            continue

        logger.info(f'Start to process {file_name}.')

        data = []

        with open(
                file=os.path.join(parameters['data_path'], file_name)
                , mode='r'
                , encoding='UTF-8') as json_file:
            lines = json_file.readlines()

            for line in tqdm(lines):
                item = json.loads(line)
                fact = item['fact']
                fact = string_process(
                    data=fact
                    , adjust_special_chars=True
                    , process_fact=True)

                fact_tensor = formatter(data=fact)

                result = model(
                    data=fact_tensor
                    , mode='serve')

                item['fact'] = string_process(
                    data=result
                    , adjust_special_chars=True)

                # data.append(str(item))
                data.append(json.dumps(item, ensure_ascii=False) + '\n')

            json_file.close()

        with open(
                file=os.path.join(parameters['output_path'], file_name)
                , mode='w'
                , encoding='UTF-8') as json_file:
            for one_data in data:
                json_file.write(one_data)

            json_file.close()

        logger.info(f'Process {file_name} successfully.')

    logger.info('Convert fact to summarization successfully.')


def get_lead_3_summary(parameters):
    logger.info('Start to use lead-3 method to get summaries.')

    data = []

    exclusion_file_names = [
        'README.md'
        , 'articles.txt'
        , 'article_sources.txt'
        , 'accusations.txt'
    ]

    for file_name in os.listdir(path=parameters['data_path']):
        if file_name in exclusion_file_names:
            continue

        logger.info(f'Start to process {file_name}.')

        data = []

        with open(
                file=os.path.join(parameters['data_path'], file_name)
                , mode='r'
                , encoding='UTF-8') as json_file:
            lines = json_file.readlines()

            for line in tqdm(lines):
                item = json.loads(line)
                fact = item['fact']
                fact = string_process(
                    data=fact
                    , adjust_special_chars=True)

                fact = re.split(r'([。，；])', fact)
                summary = ''

                for counter in range(0, len(fact)):
                    if counter >= 5:
                        break

                    summary += fact[counter]

                summary += '。'

                item['fact'] = summary

                # data.append(str(item))
                data.append(json.dumps(item, ensure_ascii=False) + '\n')

            json_file.close()

        with open(
                file=os.path.join(parameters['output_path'], file_name)
                , mode='w'
                , encoding='UTF-8') as json_file:
            for one_data in data:
                json_file.write(one_data)

            json_file.close()

        logger.info(f'Process {file_name} successfully.')

    logger.info('Convert fact to summarization successfully.')


def get_summarization_data(parameters):
    logger.info('Start to get summarization data.')

    data = load_data(data_path=parameters['data_path'])
    results = []

    for one_data in data:
        one_data = json.loads(one_data)

        text = ''
        summary = one_data['summary']

        if parameters['type'] == 'CAIL2020_sfzy':
            for item in one_data['text']:
                text += item['sentence']
        elif parameters['type'] == 'CNewSum_v2':
            for item in one_data['article']:
                text += item

        result = {
            'text': string_process(data=text, adjust_special_chars=True)
            , 'summary': string_process(data=summary, adjust_special_chars=True)
        }

        results.append(json.dumps(result, ensure_ascii=False) + '\n')

    write_back_results(parameters, data=results)

    logger.info('Get summarization data successfully.')


def load_data(data_path):
    logger.info('Start to load data.')

    data = []

    for file_name in os.listdir(data_path):
        if file_name == 'README.md':
            continue

        logger.info(f'Start to process {file_name}.')

        with open(
                file=os.path.join(data_path, file_name)
                , mode='r'
                , encoding='UTF-8') as file:
            lines = file.readlines()

            for line in lines:
                data.append(line)

            file.close()

        logger.info(f'Process {file_name} successfully.')

    logger.info('Load data successfully.')

    return data


# def convert_CNewSum_data_format(lines):
#     logger.info('Start to convert the format of CNewSum data.')

#     data = []

#     for line in lines:
#         one_data = json.loads(line)

#         one_data = {
#             'fact': one_data['summary']
#             , 'file': ''
#             , 'meta': {
#                 'relevant_articles': []
#                 , '#_relevant_articles': 0
#                 , 'relevant_articles_org': []
#                 , 'accusation': '无罪'
#                 , 'criminals': []
#                 , '#_criminals': 0
#                 , 'term_of_imprisonment': {
#                     'death_penalty': None
#                     , 'imprisonment': None
#                     , 'life_imprisonment': None
#                 }
#             }
#         }

#         one_data = (json.dumps(one_data, ensure_ascii=False) + '\n')
#         data.append(f'{one_data}\n')

#     logger.info('Convert the format of CNewSum data successfully.')

#     return data


def write_back_results(
        parameters
        , data
        , articles_list=None
        , article_sources_list=None
        , accusations_list=None):
    logger.info('Start to write back results.')
    
    if articles_list is not None \
            or article_sources_list is not None \
            or accusations_list is not None:
        write_aaa_data(
            parameters=parameters
            , articles_list=articles_list
            , article_sources_list=article_sources_list
            , accusations_list=accusations_list
        )

    write_tvt_data(
        parameters=parameters
        , data=data
    )

    logger.info('Write back results successfully.')


def write_aaa_data(
        parameters
        , articles_list
        , article_sources_list
        , accusations_list):
    name_to_data = {
        'articles.txt': articles_list
        , 'article_sources.txt': article_sources_list
        , 'accusations.txt': accusations_list
    }

    for name in name_to_data:
        logger.info(f'Start to write {name}.')

        with open(
                file=os.path.join(parameters['output_path'], name)
                , mode='w'
                , encoding='UTF-8') as txt_file:
            for data in name_to_data[name]:
                txt_file.write(f'{data}\n')

            txt_file.write('others\n')
            txt_file.close()

        logger.info(f'Write {name} successfully.')


def write_tvt_data(
        parameters
        , data):
    train_data, valid_data = train_test_split(
        data
        , random_state=parameters['random_seed']
        , train_size=parameters['train_size'])

    file_name_to_data = {'train.json': train_data, 'valid.json': valid_data}

    if parameters['generate_test_data'] is True:
        valid_data, test_data = train_test_split(
            valid_data
            , random_state=parameters['random_seed']
            , train_size=parameters['valid_size'])

        file_name_to_data['valid.json'] = valid_data
        file_name_to_data['test.json'] = test_data

    for file_name in file_name_to_data:
        logger.info(f'Start to write {file_name}.')

        with open(
                file=os.path.join(parameters['output_path'], file_name)
                , mode='w'
                , encoding='UTF-8') as json_file:
            for data in file_name_to_data[file_name]:
                json_file.write(f'{data}')

            json_file.close()

        logger.info(f'Write {file_name} successfully.')